{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, set up the training images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose set: balanced, imbalanced, all_tanks\n",
    "set = \"imbalanced\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize, don't set to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "def resize_multiple_images(src_path, dst_path):\n",
    "    # Here src_path is the location where images are saved.\n",
    "    for filename in os.listdir(src_path):\n",
    "        try:\n",
    "            img=Image.open(src_path+filename)\n",
    "            new_img = img.resize((64,64))\n",
    "            if not os.path.exists(dst_path):\n",
    "                os.makedirs(dst_path)\n",
    "            new_img.save(dst_path+filename)\n",
    "            # print('Resized, grayed, and saved {} successfully.'.format(filename))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "src_path = \"cnn_training_images/YES/\"\n",
    "dst_path = \"cnn_training_images/FINAL/\"\n",
    "resize_multiple_images(src_path, dst_path)\n",
    "\n",
    "src_path = \"cnn_training_images/NO1/\"\n",
    "dst_path = \"cnn_training_images/NO2/\"\n",
    "resize_multiple_images(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, input training data to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2175, 64, 64, 3)\n",
      "(2175,)\n",
      "Mean of y:  0.12873563218390804\n",
      "Y count_nonzero:  280\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# import images into X and Y arrays\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "def get_data(path, lab):\n",
    "    all_images_as_array=[]\n",
    "    label=[]\n",
    "    for filename in os.listdir(path):\n",
    "        if \"jpg\" in filename:\n",
    "           if \"_194\" not in filename and \"_195\" not in filename:\n",
    "                im = cv2.imread(path + filename)\n",
    "                red = im[:,:,2]\n",
    "                mean = np.mean(red)\n",
    "                std = np.std(red)\n",
    "\n",
    "                # for 2.5/97.5-percentile\n",
    "                # if mean > 133.5 and mean < 236 and std > 9.6 and std < 34:\n",
    "                # for 1/99-percentile\n",
    "                # if mean > 127.57779687499999 and mean <  246.38062499999998 and std > 7.848734708324726 and std < 38.60616608163898: \n",
    "                # for 99.9 percentile\n",
    "                if mean > 120.5 and mean < 249 and std > 5.5 and std < 40:\n",
    "\n",
    "                    try:\n",
    "                        img=cv2.imread(path + filename)\n",
    "                        all_images_as_array.append(img)\n",
    "                        label.append(lab)\n",
    "                    except:\n",
    "                        continue\n",
    "    return np.array(all_images_as_array), np.array(label)\n",
    "\n",
    "\n",
    "path_to_pos = \"training_images/\" + set + \"/YES2/\"\n",
    "path_to_neg = \"training_images/\" + set + \"/NO2/\"\n",
    "\n",
    "\n",
    "# input each class\n",
    "X1 , y1 = get_data(path_to_pos, 1)\n",
    "X0, y0 = get_data(path_to_neg, 0)\n",
    "\n",
    "# combine classes\n",
    "X = np.concatenate((X1, X0), axis = 0)\n",
    "y = np.concatenate((y1, y0), axis = 0)\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(\"Mean of y: \", np.mean(y))\n",
    "print(\"Y count_nonzero: \", np.count_nonzero(y))\n",
    "\n",
    "X = X.astype('float32')\n",
    "y = y.astype('float32')\n",
    "\n",
    "print(X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale X\n",
    "X = X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_92 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_93 (Conv2D)           (None, 62, 62, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_94 (Conv2D)           (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_95 (Conv2D)           (None, 29, 29, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_46 (MaxPooling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_96 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_97 (Conv2D)           (None, 12, 12, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 1,320,610\n",
      "Trainable params: 1,320,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Convolutional Neural Network https://machinelearningmastery.com/keras-functional-api-deep-learning/\n",
    "\n",
    "# Convolutional Neural Network\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "input_shape = (64, 64, 3)\n",
    "nClasses = 2\n",
    "\n",
    "def createModel():\n",
    "    model = Sequential()\n",
    "    # The first two layers with 32 filters of window size 3x3\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='tanh', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='tanh'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='tanh'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nClasses, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = createModel()\n",
    "model.summary()\n",
    "\n",
    "# plot_model(model, to_file='convolutional_neural_network.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2175, 2)\n",
      "(1957, 2)\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "from keras.utils import to_categorical\n",
    "y_binary = to_categorical(y)\n",
    "\n",
    "print(y_binary.shape)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.1, stratify=y_binary, random_state=0)\n",
    "print(y_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computer class weights, define metrics\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 y_train)\n",
    "print('Class weights: ',class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training process params\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "\n",
    "# Set the training configurations: optimizer, loss function, accuracy metrics\n",
    "model.compile(optimizer='rmsprop', loss=keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "class_weight = {0: .12, 1: .8}\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                     y_train,\n",
    "                     batch_size=batch_size, \n",
    "                     epochs=epochs, verbose=1, \n",
    "                     validation_data=(X_test, y_test),\n",
    "                     class_weight = class_weight\n",
    "          )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model results on the test set\n",
    "model1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 360 samples, validate on 91 samples\n",
      "Epoch 1/2\n",
      "360/360 [==============================] - 3s 9ms/sample - loss: 7.1555 - accuracy: 0.5333 - val_loss: 7.0769 - val_accuracy: 0.5385\n",
      "Epoch 2/2\n",
      "360/360 [==============================] - 0s 1ms/sample - loss: 7.1555 - accuracy: 0.5333 - val_loss: 7.0769 - val_accuracy: 0.5385\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=5000, epochs=2, validation_data = (X_test, y_test),  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computer class weights, define metrics\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y),\n",
    "                                                 y)\n",
    "print('Class weights: ',class_weights)\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input data to train array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import images into X and Y arrays\n",
    "\n",
    "nsamples, nx, ny = X.shape\n",
    "X = X.reshape((nsamples, nx*ny))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample train array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ resample train array ###########\n",
    "from collections import Counter\n",
    "print(\"Y train array: \")\n",
    "print(sorted(Counter(y_train).items()))\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "over = SMOTE(sampling_strategy = .5)\n",
    "under = RandomUnderSampler(sampling_strategy=1)\n",
    "X_train, y_train = over.fit_resample(X_train, y_train)\n",
    "X_train, y_train = under.fit_resample(X_train, y_train)\n",
    "print(\"Train data resampled.\")\n",
    "\n",
    "from collections import Counter\n",
    "print(\"Y train array: \")\n",
    "print(sorted(Counter(y_train).items()))\n",
    "\n",
    "print(\"Y test array: \")\n",
    "print(sorted(Counter(y_test).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# define and fit scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "\n",
    "################## define and run model itself, after you've found good params ########\n",
    "mlp = MLPClassifier(hidden_layer_sizes= (100, ),\n",
    "                    activation = 'relu',\n",
    "                    solver = 'lbfgs',\n",
    "                    alpha = 1e-5,\n",
    "                    learning_rate = 'constant',\n",
    "                    random_state = 0)\n",
    "\n",
    "pipe = Pipeline(steps =[('scaler',scaler) , ('MLPClassifier', mlp)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it\n",
    "cutoff = 0.5\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# test model\n",
    "# set threshold for positive output in the predict_proba line\n",
    "\n",
    "from sklearn.metrics import recall_score, accuracy_score, precision_score, f1_score\n",
    "\n",
    "y_true, y_pred = y_test, (pipe.predict_proba(X_test)[:,1] >= cutoff).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    \n",
    "    # This is the first convolution\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(80, 80)),\n",
    "    \n",
    "    tf.keras.layers.MaxPooling2D(2,2)\n",
    "    \n",
    "    # Second\n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    \n",
    "    # Third\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    # Fourth\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    # Fifth\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    # Flatten the results to feed into a DNN\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    # 512 neuron hidden layer\n",
    "    \n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    \n",
    "    \n",
    "    # Only 1 output neuron. It will contain a value from 0-1.\n",
    "    \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv] *",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
